{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haniaraslan/ASL_Gesture_recognition/blob/main/ASL_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Hand gesture recognition model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Install the MediaPipe Model Maker package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS\n"
      ],
      "metadata": {
        "id": "DWzQKATj1lD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from gtts import gTTS\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Simple End-to-End Example\n",
        "\n",
        "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
        "\n",
        "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d grassknoted/asl-alphabet"
      ],
      "metadata": {
        "id": "r3ajU8FQ6qxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(\"/content/asl-alphabet.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset-folder\")\n"
      ],
      "metadata": {
        "id": "Xx7CJWlw6td8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "dataset_path=\"/content/dataset-folder/asl_alphabet_train/asl_alphabet_train\"\n",
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)) and i != \".ipynb_checkpoints\":\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0o59OMjqmV"
      },
      "source": [
        "To better understand the dataset, plot a couple of example images for each gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx8PsrwYjvgO"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 5\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Run the example\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "**Load the dataset**\n",
        "\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTh_ZyEIeKV"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\", epochs = 10, batch_size= 60)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "**Evaluate the model performance**\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.left': False,\n",
        "    'axes.spines.bottom': False,\n",
        "    'xtick.labelbottom': False,\n",
        "    'xtick.bottom': False,\n",
        "    'ytick.labelleft': False,\n",
        "    'ytick.left': False,\n",
        "    'xtick.labeltop': False,\n",
        "    'xtick.top': False,\n",
        "    'ytick.labelright': False,\n",
        "    'ytick.right': False\n",
        "})\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "\n",
        "def display_one_image(image, title, subplot, titlesize=16):\n",
        "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "\n",
        "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
        "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
        "    # Images and labels.\n",
        "    images = [image.numpy_view() for image in images]\n",
        "    gestures = [top_gesture for (top_gesture, _) in results]\n",
        "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
        "\n",
        "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images) // rows\n",
        "\n",
        "    # Size and spacing.\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols, 1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "\n",
        "    # Display gestures and hand landmarks.\n",
        "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
        "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
        "        annotated_image = image.copy()\n",
        "\n",
        "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
        "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "          hand_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "          ])\n",
        "\n",
        "          mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks_proto,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
        "\n",
        "    # Layout.\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sKjDMOK7yzPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to speak the detected letter\n",
        "def speak_detection_gtts(letter):\n",
        "  if(letter):\n",
        "    if isinstance(letter, str):  # Ensure it's a string before passing to gTTS\n",
        "        tts = gTTS(text=letter, lang='en')\n",
        "        tts.save(f\"speech_{letter}.mp3\")\n",
        "        os.system(\"mpg321 speech.mp3\")  # Play the generated speech\n",
        "    else:\n",
        "        print(f\"Invalid input to TTS: {letter}\")\n",
        "  else:\n",
        "    print(\"No letter detected\")"
      ],
      "metadata": {
        "id": "bDD3emY_1elG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# STEP 2: Create an GestureRecognizer object.\n",
        "base_options = python.BaseOptions(model_asset_path='exported_model/gesture_recognizer.task')\n",
        "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
        "\n",
        "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
        "\n",
        "images = []\n",
        "results = []\n",
        "true_labels = []  # Ground truth labels\n",
        "predicted_labels = []  # Model's predictions\n",
        "\n",
        "for label in test_data.label_names:\n",
        "  label_dir = ''\n",
        "  if(label == 'none'):\n",
        "    label_dir = os.path.join(dataset_path, \"none\")\n",
        "  else:\n",
        "    label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames2 = os.listdir(label_dir)[:5]\n",
        "\n",
        "  for image_file_name in example_filenames2:\n",
        "    print(label_dir +'/'+ image_file_name)\n",
        "    # STEP 3: Load the input image.\n",
        "    image = mp.Image.create_from_file(label_dir +'/'+ image_file_name)\n",
        "\n",
        "    # STEP 4: Recognize gestures in the input image.\n",
        "    recognition_result = recognizer.recognize(image)\n",
        "    true_labels.append(label)\n",
        "\n",
        "    if (recognition_result.gestures):\n",
        "    # STEP 5: Process the result. In this case, visualize it.\n",
        "      images.append(image)\n",
        "      top_gesture = recognition_result.gestures[0][0]\n",
        "      predicted_labels.append(top_gesture.category_name)\n",
        "      speak_detection_gtts(top_gesture.category_name)\n",
        "      hand_landmarks = recognition_result.hand_landmarks\n",
        "      results.append((top_gesture, hand_landmarks))\n",
        "    else:\n",
        "      predicted_labels.append('none')  # Assuming 'none' for no gesture detected\n",
        "\n",
        "\n",
        "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
      ],
      "metadata": {
        "id": "3QzTUpq_zdY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hello = [\"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/M_test.jpg\",\n",
        "          \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/O_test.jpg\",\n",
        "          \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/O_test.jpg\",\n",
        "          \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/N_test.jpg\",\n",
        "          \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/space_test.jpg\",\n",
        "         \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/L_test.jpg\",\n",
        "        \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/I_test.jpg\",\n",
        "        \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/G_test.jpg\",\n",
        "        \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/H_test.jpg\",\n",
        "        \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/T_test.jpg\"]\n",
        "\n",
        "\n",
        "external_test = [ \"/content/S.jpg\",\n",
        "                  \"/content/A.jpg\",\n",
        "                 \"/content/L.jpg\",\n",
        "                 \"/content/E.jpeg\",\n",
        "                  \"/content/dataset-folder/asl_alphabet_test/asl_alphabet_test/space_test.jpg\",\n",
        "                  \"/content/N.jpg\",\n",
        "                  \"/content/E.jpeg\",\n",
        "                  \"/content/W.jpg\",]\n",
        "\n",
        "name = [\"/content/my name/H.jpg\",\n",
        "        \"/content/my name/A.jpg\",\n",
        "        \"/content/my name/N3.jpg\",\n",
        "        \"/content/my name/I.jpg\",\n",
        "        \"/content/my name/A.jpg\",\n",
        "        ]"
      ],
      "metadata": {
        "id": "osJPioQu01Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "results = []\n",
        "predicted_labels = []  # Model's predictions\n",
        "word = \"\"\n",
        "for image_file_name in hello:\n",
        "    # STEP 3: Load the input image.\n",
        "    image = mp.Image.create_from_file(image_file_name)\n",
        "    # STEP 4: Recognize gestures in the input image.\n",
        "    recognition_result = recognizer.recognize(image)\n",
        "\n",
        "    if (recognition_result.gestures):\n",
        "    # STEP 5: Process the result. In this case, visualize it.\n",
        "      images.append(image)\n",
        "      top_gesture = recognition_result.gestures[0][0]\n",
        "      predicted_labels.append(top_gesture.category_name)\n",
        "      if(top_gesture.category_name == \"space\"):\n",
        "        word+=\" \"\n",
        "      else:\n",
        "        word+=(top_gesture.category_name)\n",
        "      hand_landmarks = recognition_result.hand_landmarks\n",
        "      results.append((top_gesture, hand_landmarks))\n",
        "    else:\n",
        "      predicted_labels.append('none')  # Assuming 'none' for no gesture detected\n",
        "print(word)\n",
        "speak_detection_gtts(word.lower())"
      ],
      "metadata": {
        "id": "AdEfR1v13Y4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "**Export to Tensorflow Lite Model**\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export_tflite(\"tflite_model\")\n",
        "!ls tflite_model\n",
        "files.download('/content/ASL_Gesture_recognition/tflite_model/model.tflite')"
      ],
      "metadata": {
        "id": "zfh1KMXiJ6y5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}